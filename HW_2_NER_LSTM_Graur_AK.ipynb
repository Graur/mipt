{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eRXgS4d7pCW"
   },
   "source": [
    "## Token Classification. Практическое задание (PJ) Граур Андрей Константинович\n",
    "\n",
    "Для закрепления материала модуля вам необходимо решить задачу NER для предоставленного [датасета](https://lms.skillfactory.ru/asset-v1:SkillFactory+MFTIDS+SEP2023+type@asset+block@FactRuEval.zip), используя любые доступные вам средства. Модель должна обучаться на файле `train.txt`, валидироваться на файле `dev.txt`, а её качество необходимо оценить на файле `test.txt`.\n",
    "Для достижения наилучшего результата уделите внимание подбору гиперпарметров как в плане архитектуры, так и в плане обучения модели.\n",
    "\n",
    "Критерии оценивания проекта:\n",
    "- общее качество кода и следование PEP-8;\n",
    "- использование рекуррентных сетей;\n",
    "- использованы варианты архитектур, близкие к state of the art для данной задачи;\n",
    "- произведен подбор гиперпараметров;\n",
    "- использованы техники изменения learning rate (lr scheduler);\n",
    "- использована адекватная задаче функция потерь;\n",
    "- использованы техники регуляризации;\n",
    "- корректно проведена валидация модели;\n",
    "- использованы техники ensemble;\n",
    "- использованы дополнительные данные;\n",
    "- итоговое значение метрики качества > 0.6 (f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_UxAHDXFXuN"
   },
   "source": [
    "## 1) Импортирую необхоимдые мне зависимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcAssJKA8TKT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(1)\n",
    "from matplotlib import pyplot as plt\n",
    "import collections\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Достаю данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5WzTaKLW9FEp",
    "outputId": "6ec8e5e4-003c-47a3-b470-936ef454b9cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-04 08:54:42--  https://lms.skillfactory.ru/asset-v1:SkillFactory+MFTIDS+SEP2023+type@asset+block@FactRuEval.zip\n",
      "Resolving lms.skillfactory.ru (lms.skillfactory.ru)... 51.250.7.2\n",
      "Connecting to lms.skillfactory.ru (lms.skillfactory.ru)|51.250.7.2|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 870425 (850K) [application/x-zip-compressed]\n",
      "Saving to: ‘/content/data.zip’\n",
      "\n",
      "/content/data.zip   100%[===================>] 850.02K  1.02MB/s    in 0.8s    \n",
      "\n",
      "2023-10-04 08:54:44 (1.02 MB/s) - ‘/content/data.zip’ saved [870425/870425]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = '/content/data.zip'\n",
    "! wget https://lms.skillfactory.ru/asset-v1:SkillFactory+MFTIDS+SEP2023+type@asset+block@FactRuEval.zip -O {path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPxjgCpd_CZ_",
    "outputId": "cc88281e-ae8f-47e7-8006-69fc47fca959"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/data.zip\n",
      "  inflating: dev.txt                 \n",
      "  inflating: test.txt                \n",
      "  inflating: train.txt               \n"
     ]
    }
   ],
   "source": [
    "! unzip {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7HxfVbLudpO"
   },
   "outputs": [],
   "source": [
    "# Функция для преобразования текстовго файла в json формат\n",
    "def json_mapper(path):\n",
    "    save_path = f\"{path.split('.')[0]}\"+'.json'\n",
    "    res = dict()\n",
    "    with open(path, 'r') as in_file:\n",
    "      stripped = (line.strip('\\n') for line in in_file)\n",
    "      line_dict = dict()\n",
    "      txt = []\n",
    "      label_ = []\n",
    "      j = 0\n",
    "      for i, line in enumerate(stripped):\n",
    "        if line:\n",
    "          token_, line_ =  line.split(' ')\n",
    "          txt.append(token_)\n",
    "          label_.append(line_)\n",
    "        if not line:\n",
    "          line_dict['token'] = txt\n",
    "          line_dict['ner'] = label_\n",
    "          res.update({j: line_dict})\n",
    "          txt = []\n",
    "          label_ = []\n",
    "          line_dict = dict()\n",
    "          j+=1\n",
    "\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(res, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBVxhpRqvIhq"
   },
   "outputs": [],
   "source": [
    "json_mapper('train.txt')\n",
    "json_mapper('dev.txt')\n",
    "json_mapper('test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twWbvDyRA6yG"
   },
   "source": [
    "## 3) Проверяю и подготавливаю данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vppz2x8zNOVi"
   },
   "outputs": [],
   "source": [
    "# Read JSON file\n",
    "def read_json(path):\n",
    "  with open(path) as data_file:\n",
    "      return json.load(data_file)\n",
    "    \n",
    "train_ds = read_json('train.json')\n",
    "valid_ds = read_json('dev.json')\n",
    "test_ds = read_json('test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvXbhoFaA6yK"
   },
   "source": [
    "Эта функция фильтрует последовательности входных данных по их длине. Она принимает список последовательностей sequences и необязательный аргумент max_length, который указывает максимальную длину последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OevrN_ecns5y"
   },
   "outputs": [],
   "source": [
    "def filter_ds(sequences, max_length=256):\n",
    "    lengths = [len(sequences[str(i)]['token']) for i in range(len(sequences))]\n",
    "    print(f'Maximum length: {max(lengths)}')\n",
    "    print(f'Minimum length: {min(lengths)}')\n",
    "    print(f'Average length: {sum(lengths)/len(lengths)}')\n",
    "\n",
    "    short_sequences = []\n",
    "    for i in range(len(sequences)):\n",
    "      seq = sequences[str(i)]\n",
    "      if len(seq['token']) <= max_length:\n",
    "        short_sequences.append(seq)\n",
    "\n",
    "    print(f'% of short sequences: {100 * len(short_sequences)/len(sequences)}')\n",
    "\n",
    "    X = [[c for c in x['token']] for x in short_sequences] #[' '.join(c for c in x['token']) for x in short_sequences] #\n",
    "    y = [[c for c in y['ner']] for y in short_sequences]\n",
    "    lengths = [len(x) for x in X]\n",
    "    print(f'Maximum cleared length: {max(lengths)}')\n",
    "    return X, y\n",
    "\n",
    "short_sequences = []\n",
    "for i in range(len(train_ds)):\n",
    "  seq = train_ds[str(i)]\n",
    "  if len(seq['token']) < 256:\n",
    "    short_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4o8sjxgA6yM",
    "outputId": "32afa2f8-ccd5-478c-f3a3-4b6b09ffab70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 103\n",
      "Minimum length: 1\n",
      "Average length: 20.2285050348567\n",
      "% of short sequences: 100.0\n",
      "Maximum cleared length: 103\n",
      "\n",
      "Maximum length: 207\n",
      "Minimum length: 2\n",
      "Average length: 19.824167312161116\n",
      "% of short sequences: 99.96127033307513\n",
      "Maximum cleared length: 150\n",
      "\n",
      "Maximum length: 222\n",
      "Minimum length: 2\n",
      "Average length: 20.541440743609606\n",
      "% of short sequences: 99.96127033307513\n",
      "Maximum cleared length: 116\n"
     ]
    }
   ],
   "source": [
    "max_length = 200\n",
    "X_train, y_train = filter_ds(train_ds, max_length)\n",
    "print()\n",
    "X_valid, y_valid = filter_ds(valid_ds, max_length)\n",
    "print()\n",
    "X_test, y_test = filter_ds(test_ds, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8s7WDUMA6yP"
   },
   "source": [
    "## 3) Произвожу encoding\n",
    "\n",
    "Здесь создаю словари для преобразования токенов и меток в соответствующие индексы, используемые для обучения модели машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jxnpN9u5xqoV",
    "outputId": "96f02422-dbb4-48fc-dd40-54f1d5cfbb73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-PER', 'B-ORG', 'I-LOC', 'O', 'I-PER', 'I-ORG']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimal_frequency = 2\n",
    "train_and_valid_ds = X_train + X_valid\n",
    "func = (token for sequence in train_and_valid_ds for token in sequence)\n",
    "_token = [\"{unk}\"] + [token for token, count in collections.Counter(func).items() if count >= minimal_frequency]\n",
    "_index = collections.defaultdict(lambda: 1, {token: index for index, token in enumerate(_token)})\n",
    "_label = list(set([label for target in y_train for label in target]))\n",
    "label_with_index = {label: index for index, label in enumerate(_label)}\n",
    "_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSXgLf-RxtKi",
    "outputId": "826af6c6-6e75-4248-c86e-b08ed7091e5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 0,\n",
       " 'B-PER': 1,\n",
       " 'B-ORG': 2,\n",
       " 'I-LOC': 3,\n",
       " 'O': 4,\n",
       " 'I-PER': 5,\n",
       " 'I-ORG': 6}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_with_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PVNkGvsFA6yT",
    "outputId": "18893548-bd42-4a31-bdd8-46162aa7943a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vocabulary size:  43026\n",
      "Label vocabulary size:  7\n",
      "Maximum sequence length:  150\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(x) for x in train_and_valid_ds])\n",
    "NUM_CLASSES = len(label_with_index)\n",
    "print('Text vocabulary size: ', len(_index))\n",
    "print('Label vocabulary size: ', NUM_CLASSES)\n",
    "print('Maximum sequence length: ', max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU_6z0bVA6yV"
   },
   "outputs": [],
   "source": [
    "X_enc_train = [[_index[token] for token in sequence] for sequence in X_train]\n",
    "y_enc_train = [[label_with_index[label] for label in target] for target in y_train]\n",
    "X_enc_valid = [[_index[token] for token in sequence] for sequence in X_valid]\n",
    "y_enc_valid = [[label_with_index[label] for label in target] for target in y_valid]\n",
    "X_enc_test = [[_index[token] for token in sequence] for sequence in X_test]\n",
    "y_enc_test = [[_index[label] for label in target] for target in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xuYcaGGA6yZ"
   },
   "source": [
    "## 4) Строю модель\n",
    "\n",
    "Сначала произвожу вынесение параметров в отдельные переменные. Так же для обучения планирую использовать свою ПК с видеокартой NVIDIA GeForce GTX 1660 с поддержкой cuda. Для этого были установлены все необходимые библиотека и настроено окружение в Anakonda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLBJ6sjj8Lxt",
    "outputId": "bfba2606-0ce2-4d53-aade-d18b13d16fd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([195, 395, 396, 397, 398, 399,  33, 400, 248,   8, 401, 402, 403, 404,\n",
       "        405, 406, 248,  36])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(token2index)\n",
    "tags_size = NUM_CLASSES\n",
    "epochs = 5\n",
    "embedding_size = 128\n",
    "hidden_size = 128\n",
    "batch_size = 2048\n",
    "\n",
    "def prepare_sequence(seq):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    return torch.tensor(seq, dtype=torch.long).to(device)\n",
    "\n",
    "prepare_sequence(X_enc_valid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь определен класс `LSTMTagger`, который является моделью для тегирования последовательностей с использованием LSTM. В конструкторе класса определены слои и параметры модели, включая вложения слов, LSTM слой и линейный слой для преобразования скрытого состояния в прогнозируемое пространство меток. Метод `forward` выполняет прямой проход модели, принимая на вход последовательность слов и возвращая пространство меток. Метод `predict` использует модель для предсказания меток для заданной последовательности слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-B6DPZutl2J"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size,\n",
    "                                            embedding_dim) \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(2 * hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        return (autograd.Variable(torch.zeros(2, 1, self.hidden_dim).to(device)),\n",
    "                autograd.Variable(torch.zeros(2, 1, self.hidden_dim).to(device)))  \n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        return tag_space\n",
    "\n",
    "    def predict(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        logits = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        pred = torch.softmax(logits, dim=1)\n",
    "        pred = torch.argmax(pred, dim=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xTK5wHikvtI9",
    "outputId": "7985c921-2999-4052-8287-5a93c51167e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMTagger(\n",
       "  (word_embeddings): Embedding(35630, 128)\n",
       "  (lstm): LSTM(128, 128, bidirectional=True)\n",
       "  (hidden2tag): Linear(in_features=256, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMTagger(embedding_size,hidden_size,vocabulary_size,tags_size)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ3WhuMVA6yt"
   },
   "source": [
    "## 5) Тренирую модель\n",
    "\n",
    "### 5.1) Подготовка\n",
    "Функция `eval(X_dev, y_dev)` используется для оценки модели на тестовых данных. Внутри функции происходит следующее:\n",
    "\n",
    "- Инициализируются переменные `sum_loss`, `good` и `bad` для подсчета общей потери, количества правильных и неправильных предсказаний.\n",
    "- Модель переводится в режим оценки с помощью метода `eval()`.\n",
    "- Для каждой пары входных данных `sentence_in` и меток `targets` из тестовых данных происходит следующее:\n",
    "  - Сбрасываются градиенты и скрытое состояние модели.\n",
    "  - Выполняется прямой проход модели для получения логитов.\n",
    "  - Вычисляется потеря с использованием функции потерь `loss_function`.\n",
    "  - Применяется функция `softmax` к логитам и выполняется предсказание меток с помощью функции `argmax`.\n",
    "  - Обновляются суммарную потерю, количество правильных и неправильных предсказаний.\n",
    "- Возвращаются средняя потеря и точность модели на тестовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lwc8H8BJIK_"
   },
   "outputs": [],
   "source": [
    "def eval(X_dev,y_dev):\n",
    "    sum_loss = 0.0\n",
    "    good = 0.0\n",
    "    bad = 0.0\n",
    "    model.eval()\n",
    "    for sentence_in, targets in zip(X_dev,y_dev):\n",
    "      model.hidden = model.init_hidden()\n",
    "      logits = model(prepare_sequence(sentence_in))\n",
    "      targets = prepare_sequence(targets)\n",
    "      loss = loss_function(logits, targets)\n",
    "      preds = torch.softmax(logits, dim=1)\n",
    "      preds = torch.argmax(preds, dim=1)\n",
    "      sum_loss += loss.item()\n",
    "      correct = (preds == targets).sum().item()\n",
    "      good += correct\n",
    "      bad  += len(targets) - correct\n",
    "    return sum_loss/len(X_dev), good / (good + bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3) Непосредственно обучение\n",
    "\n",
    "Здесь происходит обучение модели на тренировочных данных с использованием метода стохастического градиентного спуска.\n",
    "\n",
    "- `eval_number` - определяет, через сколько итераций происходит оценка точности и потери на валидационных данных.\n",
    "- `loss_function` - функция потерь для вычисления потери модели.\n",
    "- `optimizer` - оптимизатор, используемый для обновления параметров модели.\n",
    "- `loss_history`, `dev_history`, `dev_acc_history` - списки для сохранения истории потери на тренировочных данных, потери на валидационных данных и точности на валидационных данных соответственно.\n",
    "- `sum_loss` - переменная для накопления потери на каждой итерации.\n",
    "- `reduce_train_size` - коэффициент, определяющий размер тренировочных данных.\n",
    "- `stop` - количество итераций, после которого обучение останавливается.\n",
    "- Внешний цикл `for epoch in range(epochs)` выполняется для каждой эпохи обучения.\n",
    "- Внутренний цикл `for sentence_in, targets in zip(X_enc_train[:stop], y_enc_train[:stop])` выполняется для каждой пары входных данных и меток из тренировочных данных.\n",
    "- Проверяется, достигнуто ли количество итераций, чтобы оценить точность и потерю на валидационных данных и сохранить результаты.\n",
    "- Градиенты обнуляются, скрытое состояние модели сбрасывается.\n",
    "- Выполняется прямой проход модели для получения предсказаний.\n",
    "- Вычисляется потеря с использованием функции потерь.\n",
    "- Обновляется суммарная потеря.\n",
    "- Выполняется обратное распространение градиентов и обновление параметров модели с помощью оптимизатора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08Ilcct5wOUY",
    "outputId": "3af59497-ea2c-4bdc-8699-d53bf415692a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train on epoch 1\n",
      "5.16% train with loss 0.6121 |   val_loss 0.5640, val_acc 0.8503\n",
      "10.33% train with loss 0.5549 |   val_loss 0.5132, val_acc 0.8590\n",
      "15.49% train with loss 0.4859 |   val_loss 0.4577, val_acc 0.8688\n",
      "\n",
      "Train on epoch 2\n",
      "20.66% train with loss 0.4299 |   val_loss 0.4233, val_acc 0.8761\n",
      "25.82% train with loss 0.4007 |   val_loss 0.4126, val_acc 0.8811\n",
      "30.98% train with loss 0.3610 |   val_loss 0.3753, val_acc 0.8870\n",
      "36.15% train with loss 0.3241 |   val_loss 0.3568, val_acc 0.8930\n",
      "\n",
      "Train on epoch 3\n",
      "41.31% train with loss 0.3033 |   val_loss 0.3343, val_acc 0.8971\n",
      "46.48% train with loss 0.2851 |   val_loss 0.3221, val_acc 0.9007\n",
      "51.64% train with loss 0.2428 |   val_loss 0.3109, val_acc 0.9068\n",
      "56.80% train with loss 0.2187 |   val_loss 0.2926, val_acc 0.9115\n",
      "\n",
      "Train on epoch 4\n",
      "61.97% train with loss 0.2069 |   val_loss 0.2866, val_acc 0.9119\n",
      "67.13% train with loss 0.1922 |   val_loss 0.3144, val_acc 0.8996\n",
      "72.30% train with loss 0.1651 |   val_loss 0.2864, val_acc 0.9081\n",
      "77.46% train with loss 0.1444 |   val_loss 0.2653, val_acc 0.9207\n",
      "\n",
      "Train on epoch 5\n",
      "82.62% train with loss 0.1391 |   val_loss 0.2597, val_acc 0.9198\n",
      "87.79% train with loss 0.1316 |   val_loss 0.2551, val_acc 0.9231\n",
      "92.95% train with loss 0.1084 |   val_loss 0.2597, val_acc 0.9254\n",
      "98.12% train with loss 0.0942 |   val_loss 0.2556, val_acc 0.9225\n"
     ]
    }
   ],
   "source": [
    "eval_number = 1000\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "dev_history = []\n",
    "loss_history = []\n",
    "dev_acc_history = []\n",
    "sum_loss = 0.0\n",
    "reduce_train_size = 2\n",
    "stop = int(len(X_enc_train)/reduce_train_size)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    i = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nTrain on epoch {epoch+1}')\n",
    "    for sentence_in, targets in zip(X_enc_train[:stop], y_enc_train[:stop]):\n",
    "        i+=1\n",
    "        if (i % eval_number == 0):\n",
    "            t = 100 * i/(epochs*stop)\n",
    "            loss_history.append(sum_loss / eval_number)\n",
    "            dev_loss, dev_acc = eval(X_enc_valid, y_enc_valid)\n",
    "            dev_history.append(dev_loss)\n",
    "            dev_acc_history.append(dev_acc)\n",
    "            print(f'{t:.2f}% train with loss {sum_loss / eval_number:.4f} |   val_loss {dev_loss:.4f}, val_acc {dev_acc:.4f}')\n",
    "            sum_loss = 0.0\n",
    "        model.zero_grad()\n",
    "        #model.train()\n",
    "        model.hidden = model.init_hidden()\n",
    "        tag_scores = model(prepare_sequence(sentence_in))\n",
    "        targets = prepare_sequence(targets)\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        sum_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KvhBMqiwWwN"
   },
   "source": [
    "## 6) Тестирую \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vm7OJhxwZcFV",
    "outputId": "030ac535-f6b1-4a54-88c4-4fccfcffa025"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24495130375448473, 0.9242464404725841)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc = eval(X_enc_test, y_enc_test)\n",
    "test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzHPKwIfYzB0",
    "outputId": "834569f0-3226-413c-af2e-1285ca265115"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-119-039af80a889d>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(seq, dtype=torch.long).to(device)\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for sentence_in in X_enc_test:\n",
    "    model.eval()\n",
    "    model.hidden = model.init_hidden()\n",
    "    logits = model(prepare_sequence(sentence_in))\n",
    "    targets = prepare_sequence(targets)\n",
    "    pred = torch.softmax(logits, dim=1)\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    preds.append(pred.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opdyTTvHwN05",
    "outputId": "17259cad-01f9-416f-d12b-b7663a5d2ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predicted classes: 48815\n",
      "Incorrect predicted classes: 4001\n",
      "0.9242464404725841\n"
     ]
    }
   ],
   "source": [
    "correct  = 0\n",
    "incorrect = 0\n",
    "for i in range(len(X_enc_test)):\n",
    "  y_pred = preds[i]\n",
    "  y_true = np.array(y_enc_test[i])\n",
    "  correct += np.nonzero(y_pred == y_true)[0].shape[0]\n",
    "  incorrect += np.nonzero(y_pred != y_true)[0].shape[0]\n",
    "\n",
    "print(\"Correct predicted classes:\", correct)\n",
    "print(\"Incorrect predicted classes:\", incorrect)\n",
    "print(correct / (correct + incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Вывод\n",
    "\n",
    "Я использовал модель LSTMTagger для тегирования последовательностей. Модель состоит из вложений слов, двунаправленного LSTM слоя и линейного слоя для преобразования скрытого состояния в пространство меток. \n",
    "Во время обучения модели используется стохастический градиентный спуск с оптимизатором SGD и функцией потерь CrossEntropyLoss. Обучение происходит в 5 эпох, где каждая эпоха состоит из нескольких итераций по батчам тренировочных данных. \n",
    "Каждые eval_number итераций происходит оценка точности и потери модели на валидационных данных. После каждой оценки результаты сохраняются в соответствующих списках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc3bfd05450aaee55c9e5edc7fb84d884c1d3e0b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKVQnF0TRVmt",
    "outputId": "4ceda7ea-ddd5-49b7-9174-df390ca1dbcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.84      0.75      0.79      1508\n",
      "       B-PER       0.64      0.67      0.65      2132\n",
      "       B-ORG       0.78      0.60      0.68      1734\n",
      "       I-LOC       0.88      0.65      0.75       342\n",
      "           O       0.95      0.97      0.96     44706\n",
      "       I-PER       0.76      0.70      0.73      1304\n",
      "       I-ORG       0.68      0.48      0.57      1090\n",
      "\n",
      "    accuracy                           0.92     52816\n",
      "   macro avg       0.79      0.69      0.73     52816\n",
      "weighted avg       0.92      0.92      0.92     52816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def flattering(sequences):\n",
    "  res = []\n",
    "  for seq in sequences:\n",
    "    res.extend(seq)\n",
    "  return res\n",
    "\n",
    "print(classification_report(flattering(y_enc_test), flattering(preds), target_names=_label))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
